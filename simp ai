import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# Load the model and tokenizer
@st.cache_resource
def load_model():
    model_name = "EleutherAI/gpt-neo-2.7B"  # Replace with another model if needed
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    return pipeline("text-generation", model=model, tokenizer=tokenizer)

chatbot = load_model()

# Streamlit UI
st.title("Hugging Face Chatbot")
st.markdown("This is a chatbot powered by a Hugging Face model. Ask it anything!")

# Chat interface
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# Input form
user_input = st.text_input("You:", "", key="user_input")

if user_input:
    # Add user message to chat history
    st.session_state.messages.append({"role": "user", "content": user_input})

    # Generate response
    with st.spinner("Thinking..."):
        response = chatbot(user_input, max_length=200, num_return_sequences=1, do_sample=True)
        bot_reply = response[0]['generated_text']

    # Add bot reply to chat history
    st.session_state.messages.append({"role": "assistant", "content": bot_reply})

    # Clear input box
    st.experimental_rerun()

# Display chat history
for message in st.session_state.messages:
    if message["role"] == "user":
        st.markdown(f"**You:** {message['content']}")
    else:
        st.markdown(f"**Bot:** {message['content']}")

st.markdown("---")
st.markdown("Powered by [Hugging Face Transformers](https://huggingface.co/) and [Streamlit](https://streamlit.io/).")
